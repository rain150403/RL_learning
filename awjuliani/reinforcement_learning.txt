增强学习可以教一个人工agent如何行动，
也可以让agent通过自身与环境的交互来学习
更有，将深度神经网络可以学习的复杂表示与增强学习agent的目标驱动的学习相结合，完成更复杂的学习。

设计强化学习，就应该思考人如何学习。

采用神经网络时会遇到的问题：
采用神经网络时，1）输入是什么（状态向量），2）输出是什么（q值向量），3）又是怎样更新的（梯度下降，反向传播如何操作）（计算当前预测的Q值与目标值之间
的差异）？
当然，以上回答只是其中的一种情况，像比如policy gradient方法，输出就是action动作的值或者概率。而像DFP这种，输入就更为复杂，会把几个变量值结合在一起，
串成一个向量。
还有其它情况有待发掘。。。

神经网络的弊端：
虽然网络学习解决了FrozenLake问题，但事实证明它不像Q-Table那么有效。虽然神经网络允许更大的灵活性，但它们是以牺牲Q-learning的稳定性为代价的。
也就是灵活性大，稳定性小，不太准，近似。当然，增加其它技巧就可以得到更加强大的网络。


预告：经历回放、冻结目标网络


输入网络的都是用来选择动作的，不要忘了，不管我们的网络多么的复杂，都是为了选择动作而生。


神经网络知识点：

1.经历回放 experience replay
神经网络需要经历存储，然后方便后面随机采样用于训练网络。要有experience buffer。

2.DQN 有两个网络，需要将Qevaluate net的参数更新到Qtarget net，这个更新过程需要好好理解。
