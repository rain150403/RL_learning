增强学习可以教一个人工agent如何行动，
也可以让agent通过自身与环境的交互来学习
更有，将深度神经网络可以学习的复杂表示与增强学习agent的目标驱动的学习相结合，完成更复杂的学习。

设计强化学习，就应该思考人如何学习。

采用神经网络时会遇到的问题：
采用神经网络时，1）输入是什么（状态向量），2）输出是什么（q值向量），3）又是怎样更新的（梯度下降，反向传播如何操作）（计算当前预测的Q值与目标值之间
的差异）？
当然，以上回答只是其中的一种情况，像比如policy gradient方法，输出就是action动作的值或者概率。而像DFP这种，输入就更为复杂，会把几个变量值结合在一起，
串成一个向量。
还有其它情况有待发掘。。。

神经网络的弊端：
虽然网络学习解决了FrozenLake问题，但事实证明它不像Q-Table那么有效。虽然神经网络允许更大的灵活性，但它们是以牺牲Q-learning的稳定性为代价的。
也就是灵活性大，稳定性小，不太准，近似。当然，增加其它技巧就可以得到更加强大的网络。


预告：经历回放、冻结目标网络


输入网络的都是用来选择动作的，不要忘了，不管我们的网络多么的复杂，都是为了选择动作而生。

def discount_rewards(r):
	""" take 1D float array of rewards and compute discounted reward """
	discounted_r = np.zeros_like(r)
	running_add = 0
	for t in reserved(xrange(0, r.size)):
		running_add = running_add * gamma + r[t]
		discounted_r[t] = running_add
	return discounted_r

神经网络知识点：

1.经历回放 experience replay
神经网络需要经历存储，然后方便后面随机采样用于训练网络。要有experience buffer。

2.DQN 有两个网络，需要将Qevaluate net的参数更新到Qtarget net，这个更新过程需要好好理解。


# 这里是一个很不错的用现成的环境做训练的例子。步骤很清晰
# loading the cartpole environment
import gym
env = gym.make('CartPole-v0')
# what happens if we try running the environment with random actions? how well do we do ? ( hint: not so well )
env.reset()
random_episodes = 0
reward_sum = 0
while random_episodes < 10:
	env.render()
	observation, reward, done, _ = env.step( np.random.randint(0, 2))
	reward_sum += reward
	if done:
		random_episodes += 1
		print( "reward for this episode was:", reward_sum )
		reward_sum = 0
		env.reset()
