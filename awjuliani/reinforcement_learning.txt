关于A的tutorials，我想我需要再仔细研究1）double-dueling-DQN 2）A3C， actor-critic这两种方法。就搞定了，当然DFP我觉得可以实践一下，关于grid world的例子可以借鉴实现一下。
——————接下来的工作：
（一）理解两个方法
（二）实践grid world
（三）看英文资料



增强学习可以教一个人工agent如何行动，
也可以让agent通过自身与环境的交互来学习
更有，将深度神经网络可以学习的复杂表示与增强学习agent的目标驱动的学习相结合，完成更复杂的学习。

设计强化学习，就应该思考人如何学习。

采用神经网络时会遇到的问题：
采用神经网络时，1）输入是什么（状态向量），2）输出是什么（q值向量），3）又是怎样更新的（梯度下降，反向传播如何操作）（计算当前预测的Q值与目标值之间
的差异）？
当然，以上回答只是其中的一种情况，像比如policy gradient方法，输出就是action动作的值或者概率。而像DFP这种，输入就更为复杂，会把几个变量值结合在一起，
串成一个向量。
还有其它情况有待发掘。。。

神经网络的弊端：
虽然网络学习解决了FrozenLake问题，但事实证明它不像Q-Table那么有效。虽然神经网络允许更大的灵活性，但它们是以牺牲Q-learning的稳定性为代价的。
也就是灵活性大，稳定性小，不太准，近似。当然，增加其它技巧就可以得到更加强大的网络。


预告：经历回放、冻结目标网络


输入网络的都是用来选择动作的，不要忘了，不管我们的网络多么的复杂，都是为了选择动作而生。

(policy gradient里有涉及)
def discount_rewards(r):
	""" take 1D float array of rewards and compute discounted reward """
	discounted_r = np.zeros_like(r)
	running_add = 0
	for t in reserved(xrange(0, r.size)):
		running_add = running_add * gamma + r[t]
		discounted_r[t] = running_add
	return discounted_r

神经网络知识点：

1.经历回放 experience replay
神经网络需要经历存储，然后方便后面随机采样用于训练网络。要有experience buffer。

"""
ExperienceBuffer is used to store a history of experiences that can be randomly drawn from when training the network.
Additional helper functions are located in helper.py
ExperienceBuffer用于存储历史经历，以便训练的时候随机选择。
"""

class ExperienceBuffer():
    def __init__(self, buffer_size = 50000):
        self.buffer = []
        self.buffer_size = buffer_size
    
    def add(self,experience):
        if len(list(self.buffer)) + len(list(experience)) >= self.buffer_size:
            self.buffer[0:(len(list(experience))+len(list(self.buffer)))-self.buffer_size] = []
        self.buffer.extend(experience)
            
    def sample(self,size):
        return np.reshape(np.array(random.sample(self.buffer,size)),[size,3])

2.DQN 有两个网络，需要将Qevaluate net的参数更新到Qtarget net，这个更新过程需要好好理解。


梯度更新也可以有更新频率的，不一定非要每一步都更新。


asynchronus advantage actor-critic(A3C)

# copies one set of variables to another.
# used to set worker network parameters to those of global network
"""把一系列的变量从一个网络拷贝到另一个，也就是用来把worker网络的参数设置到global网络上。"""

def update_target_graph(from_scope, to_scope):
	from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)
	to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)

	op_holder = []
	for from_var, to_var in zip(from_vars, to_vars):
		op_holder.append(to_var.assign(from_var))
	return op_holder


网络和agent（worker）是不一样的，前者是学习策略，后者是学生，学习如何借助这个网络变得更聪明。

像这里，有AC_network （actor-critic network）也就是演员评论家网络
还有worker， worker agent也就是动作执行者。



# 这里是一个很不错的用现成的环境做训练的例子。步骤很清晰
# loading the cartpole environment
import gym
env = gym.make('CartPole-v0')
# what happens if we try running the environment with random actions? how well do we do ? ( hint: not so well )
env.reset()
random_episodes = 0
reward_sum = 0
while random_episodes < 10:
	env.render()
	observation, reward, done, _ = env.step( np.random.randint(0, 2))
	reward_sum += reward
	if done:
		random_episodes += 1
		print( "reward for this episode was:", reward_sum )
		reward_sum = 0
		env.reset()
