1.  frozen lake 环境规则：
    FrozenLake环境由一个4x4的块组成，每个块都可能是起始块，目标块，安全冻结块或危险孔。目的是让代理学习从开始到目标，而不能进入洞穴。 在任何给定的时间，
代理人可以选择向上，向下，向左或向右移动。除了进入目标之外，每一步的奖励都是0，所以我们需要一个学习长期预期奖励的算法。

2.  gridworld
    Feel free to adjust the size of the gridworld. Making it smaller provides an easier task for our DQN agent, while making the world larger increases the challenge.你可以随意调整grid world的大小，当然越小就越简单，越大也也困难。
    The position of the three blocks is randomized every episode.还有，每一轮陷阱和目标块的位置， 还有agent的初始位置都是随机的。
    
部分可观测马尔科夫决策过程：
    Initializing the Gridworld with True limits the field of view, resulting in a partially observable MDP. Initializing it with False provides the agent with the entire environment, resulting in a fully MDP.
这个环境是可以设置的，可以设置成部分可观测，也可以整体环境都能看到。
When the agent moves through a green or red square, it is randomly moved to a new place in the environment.
如果agent通过了红色或者绿色块，那它就会到一个新的位置重新开始。


3.  n_armed bandit

here we define our bandit. for this example we are using a four-armed bandit. the pullBandit function generates a random number
from a normal distribution with a mean of 0. the lower the bandit number, the more likely a positive reward will be returned.
we want our agent to learn to always choose the arm that will give that positive reward.

在这里我们定义赌博机。对于这个例子我们用四臂赌博机。函数pullBandit（）从一个零均值的正态分布产生一个随机数字。赌博机的数字越小，就越可能得到一个
正的奖励。我们当然希望我们的agent能够学会总是选择得到正奖励的arm。

# list out our bandit arms
# currently arm 4 (index #3) is set to most often provide a positive reward
bandit_arms = [0.2, 0, -0.2, -2]
num_arms = len(bandit_arms)
def pullBandit(bandit):
	# get a random number
	result = np.random.randn(1)
	if result > bandit:
		# return a positive reward
		return 1
	else:
		# return a negative reward
		return -1

4.  CartPole-v0

this function allows us to weigh the rewards our agent recieves. in the context of the cartpole task, we want actions that kept the 
pole in the air a long time to have a large reward, and actions that contributed to the pole falling to have a decreased or negative
reward. we do this by weighing the rewards from the end of the episode, with actions at the end being seen as negative, since they 
likely contributed to the pole falling , and the episode ending. likewise, early actions are seen as more positive, since they weren't
responsible for the pole falling.

这个函数允许我们对agent得到的奖励做一个权衡。在cartpole任务的环境下，我们需要的动作是让这pole在空中停留的时间更长一些，以便得到更大的奖励，反之让它掉落的动作就会得到一个递减的或者负的奖励。
我们通过权衡在每一轮的结尾得到的奖励来选择动作，如果一个动作很有可能让pole掉落，那最终就会被视为负的，这一轮结束。同样的，一个动作不会让pole掉落，那我们就会认为它是正的。


5.  3D Doom

6.  gridworld-pic



# 这里是一个很不错的用现成的环境做训练的例子。步骤很清晰
# loading the cartpole environment
import gym
env = gym.make('CartPole-v0')
# what happens if we try running the environment with random actions? how well do we do ? ( hint: not so well )
env.reset()
random_episodes = 0
reward_sum = 0
while random_episodes < 10:
	env.render()
	observation, reward, done, _ = env.step( np.random.randint(0, 2))
	reward_sum += reward
	if done:
		random_episodes += 1
		print( "reward for this episode was:", reward_sum )
		reward_sum = 0
		env.reset()
